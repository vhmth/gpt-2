{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nScratch notebook to build as we go. All functionality folded back into\\nrelevant python files.\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Scratch notebook to build as we go. All functionality folded back into\n",
    "relevant python files.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset shards: 100%|███████████████████| 80/80 [00:01<00:00, 54.62it/s]\n",
      "\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 7973700\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 40069\n",
      "    })\n",
      "})\n",
      "\n",
      "writing /Users/vinay/src/gpt-2/./data/train.bin: 100%|█| 1024/1024 [02:18<00:00,\n",
      "writing /Users/vinay/src/gpt-2/./data/val.bin: 100%|█| 1024/1024 [00:01<00:00, 7\n"
     ]
    }
   ],
   "source": [
    "# download, split, tokenize, and save the training and test data\n",
    "!python3 ./data/prepare.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "\n",
    "\"\"\" let's build a decoder to validate we can take the prepared data back into text \"\"\"\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "train_data_bin = os.path.join('data', 'train.bin')\n",
    "val_data_bin = os.path.join('data', 'val.bin')\n",
    "\n",
    "# data_file is a path object to either the train or test data bins\n",
    "# num_docs is the number of documents you want to read from the file\n",
    "def load_data_docs(data_file, num_docs = 1):\n",
    "    docs = [] # array of docs in token format\n",
    "    current_doc = np.zeros((0,), np.uint16)\n",
    "    data_offset = 0 # where we are in the file\n",
    "    data_read_count = 1_000_000 # how many tokens to read at a time\n",
    "\n",
    "    # note each item has a size of 2 bytes (16-bits)\n",
    "    file_stats = os.stat(data_file)\n",
    "    file_size = file_stats.st_size\n",
    "    total_items = file_size / 2\n",
    "\n",
    "    data_arr = np.memmap(data_file, dtype=np.uint16, mode='r')\n",
    "\n",
    "    while data_offset < total_items and len(docs) < num_docs:\n",
    "        data_arr = np.memmap(data_file, dtype=np.uint16, mode='r')\n",
    "        data = data_arr[data_offset:data_offset + data_read_count]\n",
    "        data_offset += len(data)\n",
    "        for id in data:\n",
    "            current_doc = np.append(current_doc, id)\n",
    "            if id == enc.eot_token:\n",
    "                docs.append(current_doc)\n",
    "                current_doc = np.zeros((0,), np.uint16)\n",
    "            if len(docs) == num_docs:\n",
    "                break\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Addressing Retailer Concerns, Part 1'\n",
      "\n",
      "'ASM' #25 -- 9.99 cover price\n",
      "\n",
      "The first afternoon of the Marvel Retailer Summit (see \" Marvel Retailer Summit \") was a free-flowing discussion of retailer concerns, led by Senior Vice President – Sales and Marketing David Gabriel and Editor-in-Chief Axel Alonso. In Part 1 of this two-part article, we cover the discussion of the many changes to Marvel characters over the past few years, reboots and restarts, and expanding high-selling franchise titles. In Part 2 , we covered the discussion of talent management, securing hot talent, creating new talent, event fatigue, timing of events, and trade pricing. At the end of the event, we also asked David Gabriel some questions about the shift in the market last fall that motivated, in part, the Retailer Summit (see \" Marvel’s David Gabriel on the 2016 Market Shift \").The first topic that came up at the Summit, although it wasn’t on the agenda in that form, was the number of changes that had been made to core Marvel characters over the past few years and the fact that as of last fall, the market turned away from the new versions en masse. \"The feedback was that had changed too many characters,\" Gabriel said. \"We did very well with those characters for a number of years.\"Marvel’s upcoming initiative Generations, revealed during the Summit (see \" Marvel’s ‘Generations’ Revealed \"), is a way to set people’s minds at ease that the core characters will continue, the Marvel reps told the assembled retailers to considerable relief. \"We have not lost sight of our classic characters,\" Alonso said.Alonso noted the context of the changes, and clarified Marvel’s motivations. \"We've gone through a period where in pop culture as a whole (and you guys notice that as much as we do), there's been this massive discussion about inclusion and diversity,\" he said. \"It was a massive theme at the Academy Awards. This has swept through our culture, through Disney, and everything. We were mindful of that. But Marvel is not about politics. We are about telling stories about the world. I think we are an extension of what Stan did. When I look at what we're looking to do, we're looking to tell stories that matter in this time. That's the most important thing.\"A retailer described his experience. \"If the underlying quality of the material is good, it will do well,\" he said. \"You have obvious hits with Miles as Spider‑Man, you have it with G. Willow Wilson's Ms. Marvel, because the underlying content was good.\"\"I can also think of some other examples where something came up that pushed this boundary of things, where the content wasn't good, and it never took off. I don't think diversity is actually an issue, as long as the product is good.\"Another retailer described what he wanted to see from Marvel. \"I don't want you guys doing that stuff,\" he said of political content. \"I want you to entertain. That's the job. One of my customers even said the other day (because he knew we were coming) he wants to get stories and doesn’t mind a message, but he doesn't want to be beaten over the head with these things.\"Another retailer commented: \"When you talk about the Academy Awards, and how that was a prime topic, I look at the cold, hard reality, and I'm in business. A lot of those movies, or other things in other media, aren't really big money makers. For me, I care more about whether I'm going to sell it or not.\"Yet another pointed out that the more diverse characters brought different people into his store. \"One thing about the new books that go through my store, they don't sell the numbers that I would like,\" he said. \"They do bring in a different demographic, and I'm happy to see that money in my store.\"Alonso capped off the discussion on this topic with a personal anecdote. \"I tell this story all the time,\" he said. \"My wife is Korean, and I’ve got a Korean nephew. This Korean-American kid couldn't sleep at night. At 4:00 AM, he's looking at the ceiling because he just found out the new Hulk is a Korean-American kid. He's terrified he's going to be the next Hulk. I had to get on the phone with him and tell him, ‘you're OK. First of all, it's not a curse, and second of all, there can only be one Hulk. But put down the phone and settle down for a minute. Just let it settle in for a second.’ This little kid suddenly identified with the Hulk in a way he never had before. He'd seen the Hulk before, he liked Hulk, he liked Captain America, but now, suddenly, he can actually imagine himself as the Hulk. I think that's a cool little story.\"The discussion of changes to Marvel characters led into the topic of reboots and restarts, which Marvel has been doing fairly frequently over the past several years. Gabriel noted the sales results Marvel had been seeing until recently. \"[O]ver the past three years while we've been doing this, we've been seeing, time and time again, phenomenal, phenomenal numbers going from 100,000 to 300,000 when we've done some of these launches,\" he said. \"That number catapults us through the following 18 months on a lot of those series. We came to a point in October of last year where the industry changed on a dime.\"A retailer described the peril of relaunches. \"There's a certain bit of inertia,\" he said. \"When you start over with a number one and you're trying to restart new inertia, it's a point for people to jump off, as well as to jump on. I think you're seeing those numbers.\"Marvel’s use of sales incentives and more variants on the relaunches was also noted as one reason for their initial success. But those incentives don’t work unless they’re on #1s.\"I will be honest with you and tell you that we have tried to put those same sales incentives on the issues 24 or 25,\" Gabriel said. \"They don't get a fraction of what the #1 does. That's a problem that we all have to bear together. Once you get to issues 15, and 16, and 17 what in the world do you do to get those numbers from a 40,000, 60,000 unit book to 150,000 unit book even for one month?\"One tactic that worked on an issue that wasn’t a #1 recently was a special up-priced issue. \"When we're going from the $3.99 to the $9.99 Spider‑Man we almost tripled sales,\" Gabriel pointed out. \"Yes, there were incentives put on it. There were some variants, but to see triple sales on the $9.99 Spider‑Man book and to hear from half of the retailers saying, ‘This helped make our week,’ and then another portion of the retailers saying, ‘Shame on you Marvel for making us more money,’ we sit back. The only thing we have to look at are the numbers and comments like that. We'll go with the numbers any day, because we're interested in making us and you money.\"A problem with renumbering is that it clouds the sequence of the book format collections, which Marvel recently started addressing with chronologies to help readers find their way (see \" Marvel Clarifies Timeline for Trade-Waiters \"). \"It's not the be‑all‑end‑all of solutions,” Gabriel said. \"It helps. That's always been one of our biggest frustrations with our constant renumbering to get the numbers on the comics. The trades suffer.\"Others noted that there were factors other than the issue numbers that affected sales on the relaunches. \"There are so many different factors besides just the renumbering,\" a retailer pointed out. \"If you're renumbering and then you're bringing on talent that has a huge following behind it, or you're losing talent that had a huge following…\"One suggestion for stimulating sales on new arcs in an ongoing series was to vary the trade dress while retaining the legacy numbering. Another was to be more selective with relaunches rather than doing them across the line.Some retailers noted that the number of titles featuring a character seemed to have an impact on the success of a relaunch. \"Are there other derivatives of that book where, at that point, the customer's facing a decision, ‘OK, I'm reading two Captain America books, and one is getting renumbered, do I drop one of them or am I done with it at that point?’\"A retailer succinctly described the challenge. \"I think the mega question is, what customer do you want,\" she asked. \"Because your customer may be very different from my customer, and that's the biggest problem in the industry is getting the balance of keeping the people who've been there for 40 years, and then getting new people in who have completely different ideas. I am completely different from him [she said of her company’s co-owner]. He has been reading since he was six, and I couldn't care less if you renumbered it every six months. Because I just like to read a story, and I'm done.\"\"You nailed it,\" Alonso agreed.A related topic was the question of how many titles featuring a franchise is too many. \"There's so many different Deadpools coming out that our numbers are just going down and down and down,\" a retailer said. \"People are spread out. They don't know which ones to start with and which ones to read.\"Gabriel responded. \"Yes, we struggle with that as well,\" he said. \"What's really difficult, and Axel was pointing this out to me a lot this week, is that Deadpool Kills the Marvel Universe that we did just a couple years ago with Cullen Bunn ends up being our number one trade for the past two or three years.When you see another Deadpool miniseries coming up, it's because we're trying to make that happen. If it doesn't happen, then we do try to move on.\" Click here for Part 2.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "data_samples = load_data_docs(train_data_bin)\n",
    "for sample in data_samples:\n",
    "    print(enc.decode(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# data_file is a path object to either the train or test data bins\n",
    "# block_size is the context length feeding into the transformer\n",
    "# batch_size is the number of examples to pull\n",
    "def get_data_batch(data_file, block_size, batch_size):\n",
    "    data_arr = np.memmap(data_file, dtype=np.uint16, mode='r')\n",
    "    batch_offsets = torch.randint(len(data_arr) - block_size, (batch_size,))\n",
    "    X = torch.stack([torch.from_numpy(data_arr[i:i+block_size].astype(np.int64)) for i in batch_offsets])\n",
    "    Y = torch.stack([torch.from_numpy(data_arr[i+1:i+1+block_size].astype(np.int64)) for i in batch_offsets])\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # context length\n",
    "    vocab_size: int = 50257\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is [41137] the target: 45733\n",
      "when input is [41137, 45733] the target: 2346\n",
      "when input is [41137, 45733, 2346] the target: 11\n",
      "when input is [41137, 45733, 2346, 11] the target: 447\n",
      "when input is [41137, 45733, 2346, 11, 447] the target: 251\n",
      "when input is [41137, 45733, 2346, 11, 447, 251] the target: 531\n",
      "when input is [41137, 45733, 2346, 11, 447, 251, 531] the target: 22376\n",
      "when input is [41137, 45733, 2346, 11, 447, 251, 531, 22376] the target: 33984\n",
      "when input is [338] the target: 15070\n",
      "when input is [338, 15070] the target: 507\n",
      "when input is [338, 15070, 507] the target: 198\n",
      "when input is [338, 15070, 507, 198] the target: 198\n",
      "when input is [338, 15070, 507, 198, 198] the target: 4826\n",
      "when input is [338, 15070, 507, 198, 198, 4826] the target: 10985\n",
      "when input is [338, 15070, 507, 198, 198, 4826, 10985] the target: 550\n",
      "when input is [338, 15070, 507, 198, 198, 4826, 10985, 550] the target: 1115\n"
     ]
    }
   ],
   "source": [
    "config = GPTConfig()\n",
    "\n",
    "batch_size = 4\n",
    "xb, yb = get_data_batch(val_data_bin, config.block_size, batch_size)\n",
    "\n",
    "for b in range(2): # batch dimension\n",
    "    for t in range(8): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[41137, 45733,  2346,  ...,   508,   468,   865],\n",
      "        [  338, 15070,   507,  ...,  8649,  4326,     8],\n",
      "        [  329,   511, 11660,  ...,  3651,  2665,    13],\n",
      "        [12814,  1138,   321,  ...,   379, 12088, 28890]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        n_embd = config.n_embd\n",
    "\n",
    "        # core ffwd\n",
    "        self.ffwd_linear = nn.Linear(n_embd, 4*n_embd, bias=config.bias) # 4 * per GPT-1 paper specs of inner dimension\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "        # learned residual projection\n",
    "        self.proj_linear = nn.Linear(4*n_embd, n_embd, bias=config.bias)\n",
    "        # dropout from residual connection with ffwd block\n",
    "        self.residual_dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ffwd_linear(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.proj_linear(x)\n",
    "        x = self.residual_dropout(x)\n",
    "        return x\n",
    "\n",
    "# taken from previous lesson with bigram model\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, config, head_size):\n",
    "        super().__init__()\n",
    "        n_embd = config.n_embd\n",
    "        bias = config.bias\n",
    "        block_size = config.block_size\n",
    "\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=bias)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=bias)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=bias)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x) # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B,T,C) @ (B,C,T) -> (B,T,T)\n",
    "        wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf')) # (B,T,T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B,T,T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        n_head = config.n_head\n",
    "        n_embd = config.n_embd\n",
    "        self.heads = nn.ModuleList([Head(config, n_embd // n_head) for _ in range(n_head)])\n",
    "        self.residual_proj = nn.Linear(n_embd, n_embd) # projection layer going back into the residual pathway\n",
    "        self.residual_dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.residual_dropout(self.residual_proj(out))\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        n_embd = config.n_embd\n",
    "        bias = config.bias\n",
    "        self.ln1 = nn.LayerNorm(n_embd, bias=bias)\n",
    "        self.attention = CausalSelfAttention(config)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.ffwd = FeedForward(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # note: \"x +\" represents a residual connection\n",
    "        # you will need projection layers in the attention\n",
    "        # and ffwd blocks to learn whether this identity\n",
    "        # flow-through gradient is better in the context of\n",
    "        # the training data!\n",
    "        x = x + self.attention(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        n_embd = config.n_embd\n",
    "        block_size = config.block_size\n",
    "        vocab_size = config.vocab_size\n",
    "        n_layer = config.n_layer\n",
    "        dropout = config.dropout\n",
    "        bias = config.bias\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.blocks = nn.Sequential(*[Block(config) for _ in range(n_layer)])\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.ln_f = nn.LayerNorm(n_embd, bias=bias) # final layer norm\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        B,T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_embd = self.position_embedding_table(torch.arange(T)) # (T, C)\n",
    "        x = self.embedding_dropout(tok_emb + pos_embd) # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # conform to what pytorch expects the matrix dims to be\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(idx)\n",
    "            print(f\"logits:{logits},loss:{loss}\")\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B,C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B,C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B,T+1)\n",
    "        \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 50257])\n",
      "tensor(11.0064, grad_fn=<NllLossBackward0>)\n",
      "logits:tensor([[[ 0.9252,  0.0035, -0.4448,  ..., -0.5462, -0.6396, -0.0783]]],\n",
      "       grad_fn=<ViewBackward0>),loss:None\n",
      "! aug\n"
     ]
    }
   ],
   "source": [
    "m = GPT(config)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(enc.decode(m.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=1)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
